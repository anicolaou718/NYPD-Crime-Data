# -*- coding: utf-8 -*-
"""Project NYPD Data Final .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NhJPQbKpPoFhsiBVoG0KMSFC-uXX58zx

Dateutil.parser - A generic date/time string parser which is able to parse most known formats to represent a date and/or time
"""

import pandas as pd 
import numpy as np 
from pandas import read_csv
import matplotlib.pyplot as plt
import seaborn as sns
from dateutil.parser import parse

"""To read the csv file using parameters like header,squeeze. 
Squeeze function : If the parsed data only contains one column then return a Series.
"""

df = pd.read_csv('NYPD_Complaint_Data_Historic.csv',header=0, parse_dates=[0],index_col=0,squeeze=True)
df.head()

##from google.colab import drive
#@drive.mount('/content/drive')

"""The size of the whole dataset in numerically listed below."""

print(df.size)

"""In the below cell describe() is used to view some basic details like percentile, mean, std etc. of a nypd data frame. """

df.describe()

"""The whole dataset is divided into head and tail so that it gets easier to deal with the big dataset. In the cell 10rows from the tail of the dataset are printed for view purpose."""

print(df.tail(10))

"""Frequency based on the type of crimes that occured in the whole dataset"""

Freq=df['PD_DESC'].value_counts()
Freq

"""The info function displays the datatype each column in the nypd datset holds. Also the count of all nonnull values is displayed here.

Frequency of crimes that occur in different Precincts.The despine function removes the spines from the right and upper portion of the plot by default.
"""

sns.displot(df['ADDR_PCT_CD'])
plt.title("Frequency of Precinct Crime Occurences")
sns.despine()

"""Frequency sampling is attained by using random sampling."""

from random import sample
samp=df.sample(50)
samp.size

"""The frequency based on the type of crimes that occur in the sample of data taken"""

FreqSamp=samp['PD_DESC'].value_counts()
FreqSamp

"""Size of all the types of crimes in the sample, with the function size()."""

#Size of all the types of crimes in the sample
Freq.size

"""samp.squeeze() - Puts data together in a single dimensional array"""

sampSeries = samp.squeeze()
#Gets frequency of crimes after squeeze
sampSeries["PD_DESC"].value_counts()

sampSeries.info()

sampSeries

sampSeries.shape

sampSeries.describe()

"""Checks for missing values in columns. This function comes under data cleaning."""

sampSeries.isnull().sum()

"""Replacing these columns so that the dataset doesnt have any null cell."""

#Replacing numerical column null values with mean
median1=sampSeries['X_COORD_CD'].median()
median2=sampSeries['Y_COORD_CD'].median()
median3=sampSeries['Latitude'].median()
median4=sampSeries['Longitude'].median()

sampSeries["X_COORD_CD"].replace(np.nan, median1, inplace = True)
sampSeries["Y_COORD_CD"].replace(np.nan, median2, inplace = True)
sampSeries["Latitude"].replace(np.nan, median3, inplace = True)
sampSeries["Longitude"].replace(np.nan, median4, inplace = True)

#Checks for Null
sampSeries.isnull().sum()

#Checking for Duplicate data
# Data cleaning 
dupe = sampSeries.duplicated()
print(dupe.sum())
sampSeries[dupe]

#Replacing categorical data with the mode
mode3=sampSeries['LOC_OF_OCCUR_DESC'].mode().values[0]
mode1=sampSeries['Lat_Lon'].mode().values[0]

sampSeries['LOC_OF_OCCUR_DESC'] = sampSeries['LOC_OF_OCCUR_DESC'].replace(np.nan,mode3)
sampSeries['Lat_Lon'] = sampSeries['Lat_Lon'].replace(np.nan,mode1)
sampSeries['OFNS_DESC'] = sampSeries['OFNS_DESC'].replace(np.nan,"OFF. AGNST PUB ORD SENSBLTY &")

sampSeries=sampSeries.drop(['PARKS_NM','HADEVELOPT'], axis=1)

#Should be all 0s for null values in the dataframe
sampSeries.isnull().sum()

#In case of Duplicates - There are no duplicates
dupe1 = sampSeries.drop_duplicates(inplace=True)

"""Frequency of Crimes in the Boroughs =, by using function count()."""

sampSeries['BORO_NM'].value_counts()

"""A violin plot that shows complete crimes and compares the crimes that occur in the borough with the 3 digit codes(KY_CD) they're assigned"""

ax = sns.violinplot(x="BORO_NM", y='KY_CD', hue='CRM_ATPT_CPTD_CD',palette='Set2',data=sampSeries)

"""Frequency of Crimes that occur in the Boroughs in a large sample of the data set

---


"""

sns.countplot(df['BORO_NM'], color="blue")

"""Frequency of Crimes that occur in the Boroughs in a smaller sample of the data set"""

sns.countplot(sampSeries['BORO_NM'],color="Red")

"""Number of completed crimes vs attempted in the first 100 values of the total data set"""

sns.catplot(x="CRM_ATPT_CPTD_CD", kind='count', data=df.head(100))

"""Testing One hot encoding on Categorical columns"""

dum= pd.get_dummies(sampSeries[['OFNS_DESC', 'PD_DESC','CRM_ATPT_CPTD_CD','LAW_CAT_CD','JURIS_DESC', "BORO_NM","LOC_OF_OCCUR_DESC","PREM_TYP_DESC"]], columns = ['OFNS_DESC', 'PD_DESC','CRM_ATPT_CPTD_CD','LAW_CAT_CD','JURIS_DESC', "BORO_NM","LOC_OF_OCCUR_DESC","PREM_TYP_DESC"], prefix=['ofns_desc','pd_desc','crm_atpt_cptd_cd',"law_cat_cd","juris_desc",'boro_nm',"loc_of_occur_desc","prem_type_desc"] , drop_first = True).head()

dum.size

dum.head()

"""Top 10 crimes """

sampSeries["PD_DESC"].value_counts().head(10).plot(kind='bar')
plt.title("Top 10 Crimes")

#The Frequency of the Top 10 Crimes in the Five Boroughs
sampSeries["BORO_NM"].value_counts().head(10).plot(kind='bar')
plt.title("The Frequency of top 10 Crimes in the Five Boroughs ")

"""Looking At Data From New Years"""

chr= df[df['CMPLNT_FR_DT']=='12/31/2015']
chr.head()

"""Top 10 Crimes that occured on 12/31/2015"""

series = chr.squeeze()
series["PD_DESC"].value_counts().head(10).plot(kind='bar')
plt.title("Top 10 Crimes that occured on 12/31/2015")

"""Number of Crimes based on the Boroughs on New Years Eve"""

series['BORO_NM'].value_counts().head(10)

"""The Frequency of Crimes in the Five Boroughs on 12/31/2015"""

series["BORO_NM"].value_counts().head(10).plot(kind='bar')
plt.title("The Frequency of Crimes in the Five Boroughs on 12/31/2015")

"""Checks Top 10 Used Jurisdiction for the Crimes during New Years Eve"""

series['JURIS_DESC'].value_counts().head(10)

"""The Frequency of Crimes handled by Jurisdiction on 12/31/2015"""

series["JURIS_DESC"].value_counts().head(10).plot(kind='bar')
plt.title("The Frequency of Crimes handled by Jurisdiction")

"""END OF EDA FOR DATA

**Starting DATA Processing for Variables to be used in Machine Learning Algorithms**

Removing Dates with Incorrect Years for Date Conversion
"""

df['CMPLNT_FR_DT']=pd.to_datetime(df['CMPLNT_FR_DT'], errors = 'coerce')

df['CMPLNT_FR_TM']=pd.to_datetime(df['CMPLNT_FR_TM'] , format = '%H:%M:%S').dt.hour

#Confirms that 'CMPLNT_FR_DT' was changed to date time
df.info()

df_w=df.copy(deep=True)
df_d=df.copy(deep=True)

df_d=df_d[df_d.CMPLNT_FR_DT.dt.year.eq(2015)]
df_w=df_w[df_w.CMPLNT_FR_DT.dt.year.eq(2015)]

"""Creating Seasons Column"""

season = (df_w['CMPLNT_FR_DT'].dt.month - 1) // 3
season += (df_w['CMPLNT_FR_DT'].dt.month == 3)&(df_w['CMPLNT_FR_DT'].dt.day>=20)
season += (df_w['CMPLNT_FR_DT'].dt.month == 6)&(df_w['CMPLNT_FR_DT'].dt.day>=21)
season += (df_w['CMPLNT_FR_DT'].dt.month == 9)&(df_w['CMPLNT_FR_DT'].dt.day>=23)
season -= 3*((df_w['CMPLNT_FR_DT'].dt.month == 12)&(df_w['CMPLNT_FR_DT'].dt.day>=21)).astype(int)

season

#Creates new column showing seasons
df_w['Seasons']=season

#Creates new column showing seasons
df_d['Seasons']=season

"""Dropping Columns"""

df_w=df_w.drop(['CMPLNT_TO_DT','CMPLNT_TO_TM','RPT_DT', 'KY_CD','PD_CD','PD_DESC','CRM_ATPT_CPTD_CD','ADDR_PCT_CD','LOC_OF_OCCUR_DESC','JURIS_DESC','PREM_TYP_DESC','PARKS_NM','HADEVELOPT',"X_COORD_CD",'Y_COORD_CD','Lat_Lon'], axis=1)

"""Dealing with Null Values"""

df_w.isnull().sum()

median1=df_w['Latitude'].median()
median2=df_w['Longitude'].median()

df_w["Latitude"].replace(np.nan, median1, inplace = True)
df_w["Longitude"].replace(np.nan, median2, inplace = True)

df_w.isnull().sum()

df_w['OFNS_DESC'].value_counts().head(12)

df_w['LAW_CAT_CD'].value_counts()

print (df_w.size)

"""One Hot Encoding For Boroughs and Level of Offense"""

#Dummies for Boros
one_hot=pd.get_dummies(df_w['BORO_NM'])
df_w=df_w.drop('BORO_NM',axis=1)
df_w=df_w.join(one_hot)

df_w.info()

# Dummies for Level of Offense Column
one_hotL=pd.get_dummies(df_w['LAW_CAT_CD'])
df_w=df_w.drop('LAW_CAT_CD',axis=1)
df_w=df_w.join(one_hotL)

#Moved the complaint Time so all the number values would be at the end.
column_to_move = df_w.pop("CMPLNT_FR_TM")
df_w.insert(5, "CMPLNT_FR_TM", column_to_move )

#Rename Some Columns
df_w.rename(columns={'CMPLNT_FR_DT': 'ComplaintDate', 'OFNS_DESC ': 'OffenseDesc','LAW_CAT_CD ': 'LevelOfOffense','CMPLNT_FR_TM': 'TimeOfComplaint', 'CRIMINAL MISCHIEF & RELATED OF':'CRIMINAL MISCHIEF',
                          'ASSAULT 3 & RELATED OFFENSES':'ASSAULT 3','OFF. AGNST PUB ORD SENSBLTY &':'OFF. AGNST PUB ORD'}, inplace=True)

#Rename Some Columns
df_d.rename(columns={'CMPLNT_FR_DT': 'ComplaintDate', 'OFNS_DESC ': 'OffenseDesc','LAW_CAT_CD ': 'LevelOfOffense','CMPLNT_FR_TM': 'TimeOfComplaint', 'CRIMINAL MISCHIEF & RELATED OF':'CRIMINAL MISCHIEF',
                          'ASSAULT 3 & RELATED OFFENSES':'ASSAULT 3','OFF. AGNST PUB ORD SENSBLTY &':'OFF. AGNST PUB ORD'}, inplace=True)

"""Outlier Check"""

import warnings
warnings.filterwarnings('ignore')
plt.figure(figsize=(16,5))
plt.subplot(1,2,1)
sns.distplot(df_w['Latitude'])
plt.subplot(1,2,2)
sns.distplot(df_w['TimeOfComplaint'])
plt.show()

from numpy import mean
from numpy import std
data_mean, data_std = mean(df_w['TimeOfComplaint']), std(df_w['TimeOfComplaint'])

cut_off = data_std * 3
lower, upper = data_mean - cut_off, data_mean + cut_off

outliers = [ x for x in df_w['TimeOfComplaint'] if x < lower or x > upper]

print('Identified outliers: %d' % len(outliers))
outliers_removed = [x for x in df_w['TimeOfComplaint'] if x >= lower and x <= upper]
print('Non-outlier observations: %d' % len(outliers_removed))

sns.boxplot(df_w['TimeOfComplaint'])

sns.boxplot(df_w['Latitude'])

#Removing Columns that aren't necessary
df_w=df_w.drop(['ComplaintDate','OFNS_DESC','VIOLATION','BRONX'],axis=1) #','STATEN ISLAND'

"""Sampling Data"""

#Sampling the dataset
from random import sample
sampdf=df_w.sample(20000)

#All of The Columns are number values and can be used for the machine learning algos
df_w.info()

"""Splitting Data and Assigning Variable Values"""

X = sampdf.drop(['MISDEMEANOR'], axis=1)
y = sampdf['MISDEMEANOR'] # The target variable - Only one column

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

X_train.shape, X_test.shape

cols = X_train.columns

#Standardizing Variables
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)

X_test = scaler.transform(X_test)

X_train = pd.DataFrame(X_train, columns=[cols])

X_test = pd.DataFrame(X_test, columns=[cols])

X_train.describe()

df_w.Seasons.value_counts()

df_w.head()

"""A**LGORITHM IMPLEMENTATION STARTS: SVC, LOGISTIC REGRESSION, RANDOM TREE, DECISION TREE, KNN**

Default SVC
"""

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
svc=SVC()

svc.fit(X_train,y_train)

y_pred=svc.predict(X_test)

"""Accuracy of Default SVC Model"""

print('Model accuracy score with default hyperparameters: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))

"""SVC with Poly Kernel and C=10"""

poly_svc=SVC(kernel='poly', C=10.0) 
poly_svc.fit(X_train,y_train)
y_pred=poly_svc.predict(X_test)

"""Testing Numerous Accuracy Test for Consistency"""

print('Accuracy Score with poly kernel and C=10.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test,y_pred)
accuracy = float(cm.diagonal().sum())/len(y_test)
print("\SVC Accuracy using Confusion Matrix : ", accuracy)

cm

#MAE
from sklearn.metrics import mean_absolute_error
mae = mean_absolute_error(y_test, y_pred)
print("\nSVC Accuracy using MAE : ", mae)

#MAPE
from sklearn.metrics import mean_absolute_percentage_error
mape = mean_absolute_percentage_error(y_test, y_pred)
print("\nSVC Accuracy using MAPE : ", mape)

"""Gaussian SVM with rbf as Kernel"""

svclassifier = SVC(kernel='rbf')
svclassifier.fit(X_train, y_train)
y_pred = svclassifier.predict(X_test)

"""Accuracy of rbf Kernel SVC"""

from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""Logistic Regression"""

from sklearn.linear_model import LogisticRegression

model = LogisticRegression(solver='liblinear', random_state=0)

model.fit(X, y)

"""Analyzing Features of Logistic Regression Model"""

model.classes_

model.intercept_

model.coef_

"""Testing Mutliple Accuracy Models for Consistency"""

model.predict_proba(X)

predLog=model.predict(X)
predLog

confusion_matrix(y, model.predict(X))

print(classification_report(y, predLog))

# Analyzing Feature Importance of Logistic Regression Model
model=LogisticRegression(random_state=1)
model.fit(X,y)
feature_importance=pd.DataFrame({'feature':list(X.columns),'feature_importance':[abs(i) for i in model.coef_[0]]})
feature_importance.sort_values('feature_importance',ascending=False)

"""Decision Tree Analysis """

from sklearn import metrics
from sklearn import tree

#Creating the variable for Decision Tree Classifier object
classifier = tree.DecisionTreeClassifier(max_depth = 10,criterion='entropy')  #This parameter allows us to use the different-different attribute selection measure. Supported criteria are “gini” for the Gini index and “entropy” for the information gain.
#Fit X_train and Y_train to the classifier
classifier.fit(X_train, y_train)
#Making the prediction on X_test
y_pred = classifier.predict(X_test)
#Making the prediction on X_train
y_pred1 = classifier.predict(X_train)
#comparing actual response values (y_train with predicted response values (y_pred1)
DF_Train =  metrics.accuracy_score(y_train, y_pred1)*100
print("Decision Tree model accuracy(in %) for Training:",DF_Train)
#comparing actual response values (y_test) with predicted response values (y_pred)
DF_Test = metrics.accuracy_score(y_test, y_pred)*100
print("Descision Tree  model accuracy(in %) for Testing:", DF_Test)

##Confusion Matrix is the number of correct and incorrect predictions made by a classifier
result = confusion_matrix(y_test, y_pred)
print("Confusion Matrix for Decision Tree:")
print(result)
#Getting the classification report
result1 = classification_report(y_test, y_pred)
print("Classification Report for Decision Tree:",)
print (result1)
#Overall accuracy
result2 = accuracy_score(y_test,y_pred)
print("Overall Accuracy For Decision Tree:",result2)

FP = result.sum(axis=0) - np.diag(result) 
FN = result.sum(axis=1) - np.diag(result)
TP = np.diag(result)
TN = result.sum() - (FP + FN + TP)
FP = FP.astype(float)
FN = FN.astype(float)
TP = TP.astype(float)
TN = TN.astype(float)
total=sum(sum(result))
# Sensitivity, hit rate, recall, or true positive rate
TPR = ((TP/(TP+FN))).mean()*100
# Specificity or true negative rate
TNR = ((TN/(TN+FP))).mean()*100 

print('Sensitivity:' ,TPR)
print('Specificity: ' ,TNR)

from statsmodels.stats.outliers_influence import variance_inflation_factor
vif_data = [variance_inflation_factor(df_w.values, i)
                          for i in range(len(df_w.columns))]

vif_data

df_w.columns

"""Random Forest Classifier """

from sklearn.ensemble import RandomForestClassifier
rf_clf = RandomForestClassifier(criterion='entropy')   
rf_clf.fit(X_train,y_train)

y_predict = rf_clf.predict(X_test)

from sklearn.metrics import accuracy_score,classification_report,confusion_matrix
accuracy_score(y_test,y_predict)

"""K Nearest Neighbour"""

from sklearn.neighbors import KNeighborsClassifier

knnClass = KNeighborsClassifier(p=1,n_neighbors=15)
 
knnClass.fit(X_train, y_train)
 
predictKNN =knnClass.predict(X_test)

predictKNN

accuracy_score(y_test,predictKNN)

fpr, tpr, thresholds = roc_curve(y_test, predictKNN)
fig, ax = plt.subplots()
ax.plot(fpr, tpr)
ax.plot([0, 1], [0, 1], transform=ax.transAxes, ls="--", c=".3")
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.rcParams['font.size'] = 12
plt.title('ROC curve for Crime classifier')
plt.xlabel('False Positives')
plt.ylabel('True Positives')
plt.grid(True)

auc(fpr, tpr)

sns.scatterplot(data=df_d, x='BORO_NM',y='Seasons', hue='LAW_CAT_CD')